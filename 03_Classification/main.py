"""
äº¤äº’åŠ¨ä½œå›¾+åŒ…å­—èŠ‚åºåˆ—
"""
import sys
sys.path.append('.')
import torch
import ast
import random
from collections import Counter,defaultdict
from torch.utils.data import DataLoader, Dataset, DistributedSampler
import datetime
from network import create_model,GraphSAGENetwork
import json
from torch.nn.utils.rnn import pad_sequence
import time
import numpy as np
import pandas as pd
from torch.utils.data import Subset
from torch_geometric.utils import to_dense_adj
from GraphNet import GraphNet, GATGraphNet,SAGEGraphNet
from NewGraphDataset import GraphMitDataset
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,classification_report
# from DrawPicture import DrawPic
from sklearn.model_selection import  StratifiedKFold,KFold
import os
from tqdm import tqdm
import tiktoken

class GraphTokenMainClass(object):
    def __init__(self, data, num_classes, test_data,multi_model,valid_data,flag):
        #self.src_graph_model = SAGEGraphNet(num_classes).to(device)
        self.multi_model = multi_model
        self.data = data  # è®­ç»ƒé›†
        self.test_data = test_data
        self.valid_data = valid_data
        self.flag=flag
        self.accumulation_step=1

    def train(self):

        optimizier = torch.optim.Adam(self.multi_model.parameters(), lr=1e-4, weight_decay=1e-4)
        criterion = torch.nn.CrossEntropyLoss().to(device)
        self.multi_model.train()

        train_batch_cnt = 0
        for i,data_batch in enumerate(self.data):
            #print('train_batch_cnt '+str(train_batch_cnt))
            train_batch_cnt += 1

            target = data_batch["labels"].to(device)  # ç¡®ä¿æ ‡ç­¾æ•°æ®åœ¨ GPU ä¸Š
            target=target-1

            # ç¡®ä¿æ‰€æœ‰è¾“å…¥æ•°æ®éƒ½ç§»åŠ¨åˆ° GPU
            node_features = data_batch["node_features"].to(device)
            adj_matrices = data_batch["adj_matrices"].to(device)
            token_data = data_batch["token_data"].to(device)
            # print(node_features.device)
            # print(adj_matrices.device,)

            out = self.multi_model(node_features, adj_matrices, token_data, self.flag,device)
            # _, predicted = torch.max(out, dim=1)
            # print(predicted)
            # print(target.dtype)
            optimizier.zero_grad()
            target = target.long()
            loss = criterion(out, target)
            # print(f'batch:{i+1},training_loss:{loss.item():.4f}')
            with open(
                    r"/home/phc/PycharmProjects/CIBG_encrtypted_traffic_service_classification/03_Classification/results/USTC/4class/loss.txt",
                    "a") as f:
                f.write(f'Batch {i + 1}, Training Loss: {loss.item():.4f}\n')

            loss=loss/self.accumulation_step

            if (i+1)%self.accumulation_step==0:

                loss.backward()
                optimizier.step()


    def predict(self):
        self.multi_model.eval()
        correct = 0
        test_batch_cnt = 0
        criterion = torch.nn.CrossEntropyLoss().to(device)
        for i,data_batch in enumerate(self.valid_data):

            # print('test_batch_cnt '+str(test_batch_cnt))
            test_batch_cnt += 1
            target=data_batch["labels"].to(device)
            target=target-1
            # ç¡®ä¿æ‰€æœ‰è¾“å…¥æ•°æ®éƒ½ç§»åŠ¨åˆ° GPU
            node_features = data_batch["node_features"].to(device)
            adj_matrices = data_batch["adj_matrices"].to(device)
            token_data = data_batch["token_data"].to(device)

            out = self.multi_model(node_features, adj_matrices, token_data, self.flag,device)
            pred = out.argmax(dim=1)  # ä½¿ç”¨æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«
            correct += int((pred == target).sum())  # æ£€æŸ¥çœŸå®æ ‡ç­¾
            loss = criterion(out, target)
            # print(f'batch:{i+1},testing_loss:{loss.item():.4f}')
            with open(
                    r"/home/phc/PycharmProjects/CIBG_encrtypted_traffic_service_classification/03_Classification/results/USTC/4class/loss.txt",
                    "a") as f:
                f.write(f'Batch {i + 1}, Validing Loss: {loss.item():.4f}\n')

        return correct / len(self.valid_data.dataset)

    def save_model(self, model_path):
        torch.save(self.multi_model, model_path)

    def test(self, model_path):
        self.multi_model = torch.load(model_path,weights_only=False)
        self.multi_model.eval()
        correct = 0
        true_list = []
        pred_list = []
        pred_score = []  # æ¦‚ç‡å‘é‡ï¼Œç”¨äºç”»pr roc
        criterion = torch.nn.CrossEntropyLoss().to(device)
        for i,data_batch in enumerate(self.test_data):

            target=data_batch["labels"].to(device)
            target=target-1
            # ç¡®ä¿æ‰€æœ‰è¾“å…¥æ•°æ®éƒ½ç§»åŠ¨åˆ° GPU
            node_features = data_batch["node_features"].to(device)
            adj_matrices = data_batch["adj_matrices"].to(device)
            token_data = data_batch["token_data"].to(device)

            out = self.multi_model(node_features, adj_matrices, token_data, self.flag,device)
            pred_score.extend(out.tolist())
            pred = out.argmax(dim=1)  # ä½¿ç”¨æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«
            correct += int((pred == target).sum())  # æ£€æŸ¥çœŸå®æ ‡ç­¾
            true_list.append(target.tolist())
            pred_list.append(pred.tolist())
            loss = criterion(out, target)
            # print(f'batch:{i + 1},validing_loss:{loss.item():.4f}')
            with open(
                    r"/home/phc/PycharmProjects/CIBG_encrtypted_traffic_service_classification/03_Classification/results/USTC/4class/loss.txt",
                    "a") as f:
                f.write(f'Batch {i + 1}, Testing Loss: {loss.item():.4f}\n')

        return correct / len(self.test_data.dataset), true_list, pred_list, pred_score


class GraphTokenDataset(Dataset):
    def __init__(self, data, slices, token_tensors):
        self.data = data
        self.slices = slices
        self.token_data = token_tensors

        # è®¡ç®—å›¾çš„æ•°é‡
        self.num_graphs = len(slices['y']) - 1

        assert len(self.token_data) == self.num_graphs, "token_data å’Œ labels é•¿åº¦ä¸åŒ¹é…ï¼"

    def __len__(self):
        return self.num_graphs

    def __getitem__(self, idx):
        # è·å–å½“å‰å›¾çš„ x å’Œ edge_index çš„åˆ‡ç‰‡
        start_x, end_x = self.slices['x'][idx].item(), self.slices['x'][idx + 1].item()
        start_edge, end_edge = self.slices['edge_index'][idx].item(), self.slices['edge_index'][idx + 1].item()

        # æå–è¯¥å›¾çš„èŠ‚ç‚¹ç‰¹å¾
        node_features = self.data.x[start_x:end_x]

        # æå–è¯¥å›¾çš„é‚»æ¥çŸ©é˜µ
        edge_index = self.data.edge_index[:, start_edge:end_edge]
        adj_matrix = to_dense_adj(edge_index, max_num_nodes=len(node_features)).squeeze(0)

        # æå–è¯¥å›¾çš„ token_data å’Œ label
        token = self.token_data[idx]
        label = self.data.y[idx]

        return {
            "node_features": node_features,
            "adj_matrix": adj_matrix,
            "token_data": token,
            "label": label
        }
def major_exp_multi_classify(multimodel,name,flag):
    batch_size = 20
    num_epoch_list =list(range(20,21,10))
    num_counts=31

    for num_epoch in num_epoch_list:
        # è·å¾—å›¾ç»“æ„æ•°æ®
        graph_structure = GraphMitDataset(
            root=r'/home/phc/PycharmProjects/CIBG_encrtypted_traffic_service_classification/03_Classification/interaction_matters_data/graph/USTC/4class',
            label_dict={f'label{i}': i for i in range(1, 15)},
            node_counts=num_counts)

        data_src = pd.read_csv(r'/home/phc/PycharmProjects/CIBG_encrtypted_traffic_service_classification/01_feature_extract/mit/merge_USTC_4_class_hex_data_mit.csv.csv')

        # è¯»å–å›¾ç»“æ„æ•°æ®è€ƒè™‘çš„æµ
        valid_indices_df = pd.read_csv(r'/home/phc/PycharmProjects/CIBG_encrtypted_traffic_service_classification/03_Classification/interaction_matters_data/graph/USTC/4class/valid_indices.csv')
        valid_indices = valid_indices_df['valid_indices']
        selected_data = data_src.loc[valid_indices]

        hex_data = selected_data['udps.bi_payload_hex']

        # è½¬æ¢åŒ…å­—èŠ‚åºåˆ—ä¸ºåˆ—è¡¨
        hex_data_list = hex_data.apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)

        # åˆå§‹åŒ– tiktoken ç¼–ç å™¨
        tokenizer = tiktoken.get_encoding("cl100k_base")

        # å­˜å‚¨æœ€ç»ˆçš„ token tensor
        token_tensors = []

        for hex in hex_data_list:
            # è·å–å‰ num_node ä¸ªæ•°æ®åŒ…çš„ hex åºåˆ—
            # å¦‚æœæ•°æ®åŒ…æ•°é‡å°‘äº num_nodeï¼Œå°±ç›´æ¥é€‰å–å…¨éƒ¨
            selected_packet = hex[:num_counts]  # æˆªå–å‰ num_node ä¸ªæ•°æ®åŒ…çš„ hex å­—ç¬¦ä¸²

            truncated_packet=[pkt[:78] for pkt in selected_packet]

            # å°†é€‰ä¸­çš„ hex å­—ç¬¦ä¸²æ‹¼æ¥åœ¨ä¸€èµ·
            hex_string = ''.join(truncated_packet)  # æ‹¼æ¥å­—ç¬¦ä¸²

            # ç”¨ tokenizer è¿›è¡Œç¼–ç 
            token_ids = tokenizer.encode(hex_string)  # ç¼–ç ä¸º token ids

            # å°†ç¼–ç ç»“æœè½¬åŒ–ä¸º tensor
            token_tensor = torch.tensor(token_ids, dtype=torch.long)

            # å°† token tensor æ·»åŠ åˆ°æœ€ç»ˆåˆ—è¡¨
            token_tensors.append(token_tensor)


        """
        å¼€å§‹åœ¨datasetä¸­å°±æ‹†å¼€å½’çº³å›¾ç»“æ„æ•°æ®
        """
        dataset = GraphTokenDataset(graph_structure.data,graph_structure.slices, token_tensors)
        # # å®šä¹‰è®¡æ•°å™¨
        # count_node_features = 0
        # count_adj_matrix = 0
        # for i in range(len(dataset)):
        #     sample = dataset[i]  # è®¿é—®ç¬¬ä¸€ä¸ªæ ·æœ¬
        #     node_features = sample["node_features"]
        #     adj_matrix = sample["adj_matrix"]
        #     token = sample["token_data"]
        #     label = sample["label"]
        #
        #     #æ£€æŸ¥èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
        #     if node_features.shape != torch.Size([20, 13]):
        #         count_node_features += 1
        #         print(node_features.shape)
        #
        #     # æ£€æŸ¥é‚»æ¥çŸ©é˜µç»´åº¦
        #     if adj_matrix.shape != torch.Size([20, 20]):
        #         count_adj_matrix += 1
        #         print(adj_matrix.shape)
        #
        # # è¾“å‡ºç¬¦åˆæ¡ä»¶çš„æ ·æœ¬æ•°é‡
        # print(f"ä¸ç¬¦åˆèŠ‚ç‚¹ç‰¹å¾ç»´åº¦ [20, 13] çš„æ ·æœ¬æ•°: {count_node_features}")
        # print(f"ä¸ç¬¦åˆé‚»æ¥çŸ©é˜µç»´åº¦ [20, 20] çš„æ ·æœ¬æ•°: {count_adj_matrix}")
        labels = [dataset.data.y[i].item() for i in range(dataset.num_graphs)]
        # print('labels',labels)
        # print(len(labels))
        best_overall_acc = 0
        best_model_path = ""

        # å®šä¹‰åé‡äº¤å‰éªŒè¯
        kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

        true_all = []
        pred_all = []
        pred_scr_all = []
        index_save_path = r'/home/phc/PycharmProjects/CIBG_encrtypted_traffic_service_classification/03_Classification/results/USTC/4class/split_indices'
        os.makedirs(index_save_path, exist_ok=True)
        split_iter = kf.split(np.arange(len(dataset)), labels)
        for fold, (train_idx, test_idx) in enumerate(
                tqdm(split_iter, total=kf.get_n_splits(), desc="ğŸŒŸ K-Fold Progress")):
            print(f"\n\n========== Fold {fold + 1} ==========")

            pred_scr_all = []

            label_to_indices = defaultdict(list)
            for idx in train_idx:
                label = labels[idx]
                label_to_indices[label].append(idx)

            max_per_class =5000
            selected_train_indices = []
            for label, idx_list in label_to_indices.items():
                if len(idx_list) > max_per_class:
                    selected = random.sample(idx_list, max_per_class)
                else:
                    selected = idx_list
                selected_train_indices.extend(selected)

            val_ratio = 0.1
            train_size = int((1 - val_ratio) * len(selected_train_indices))
            val_size = len(selected_train_indices) - train_size

            generator = torch.Generator().manual_seed(42)
            train_data, valid_data = torch.utils.data.random_split(
                Subset(dataset, selected_train_indices),
                [train_size, val_size],
                generator=generator
            )

            test_subset = Subset(dataset, test_idx)

            for i,sample in enumerate(train_data):
                if sample['token_data'].shape[0]==0:
                    print('------------------------------------token kong-------------------------------',sample['token_data'])

            def count_labels(data, name):
                # å…ˆå–å‡ºæ‰€æœ‰æ ‡ç­¾
                labels = [sample["label"] for sample in data]
                # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«å‡ºç°çš„æ¬¡æ•°
                counts = Counter(labels)

                # æ‰“å°æ±‡æ€»ä¿¡æ¯
                total = len(labels)
                print(f"\n{name} é›†ä¸­å„ç±»åˆ«æ ·æœ¬æ•°é‡ï¼ˆå…± {total} æ¡ï¼‰ï¼š")
                # for label, count in sorted(counts.items()):
                #     print(f"  ç±»åˆ« {label}: {count} æ¡")

            # æ”¾åœ¨ train/val/test åˆ’åˆ†ä¹‹å
            count_labels(train_data, "è®­ç»ƒ")
            count_labels(valid_data, "éªŒè¯")
            count_labels(test_subset, "æµ‹è¯•")

            # è·å–åŸå§‹ data_src ç´¢å¼•ï¼ˆè¿™äº›éƒ½æ˜¯ valid_indices ä¸­çš„å­é›†ï¼‰
            train_index_in_data_src = valid_indices.iloc[train_data.indices].tolist()
            val_index_in_data_src = valid_indices.iloc[valid_data.indices].tolist()
            test_index_in_data_src = valid_indices.iloc[test_idx].tolist()

            # ä¿å­˜ä¸º JSON
            split_record = {
                "fold": fold,
                "train_indices": train_index_in_data_src,
                "val_indices": val_index_in_data_src,
                "test_indices": test_index_in_data_src
            }
            with open(os.path.join(index_save_path, f"fold_{fold}_indices.json"), "w") as f:
                json.dump(split_record, f, indent=2)
            #åˆ›å»ºæ•°æ®åŠ è½½å™¨
            def collate_fn1(batch):
                # print("Received batch with size:", len(batch))
                node_features_list = [data["node_features"] for data in batch]
                adj_matrices_list = [data["adj_matrix"] for data in batch]
                labels = torch.tensor([data["label"] for data in batch], dtype=torch.long)


                # **å¡«å…… token_data**
                token_data_list = [data["token_data"] for data in batch]
                token_data_padded = pad_sequence(token_data_list, batch_first=True, padding_value=0)
                # print(f"Token data padded shape: {token_data_padded.shape}")

                # **è®¡ç®—æœ€å¤§èŠ‚ç‚¹æ•°ï¼Œå¹¶å¡«å……é‚»æ¥çŸ©é˜µå’ŒèŠ‚ç‚¹ç‰¹å¾**
                max_nodes = max(nf.shape[0] for nf in node_features_list)

                # åˆ›å»ºå¡«å……åçš„ batch
                node_padded = torch.zeros(len(node_features_list), max_nodes, node_features_list[0].shape[-1])
                adj_padded = torch.zeros(len(adj_matrices_list), max_nodes, max_nodes)

                for i, (node_features, adj_matrix) in enumerate(zip(node_features_list, adj_matrices_list)):
                    num_nodes = node_features.shape[0]
                    node_padded[i, :num_nodes, :] = node_features
                    adj_padded[i, :num_nodes, :num_nodes] = adj_matrix

                return {
                    "node_features": node_padded,
                    "adj_matrices": adj_padded,
                    "token_data": token_data_padded,
                    "labels": labels
                }

            def seed_worker(worker_id):
                worker_seed = torch.initial_seed() % 2 ** 32
                np.random.seed(worker_seed)
                random.seed(worker_seed)


            train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn1,worker_init_fn=seed_worker,generator=generator)
            valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn1)
            test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn1)
            # # å»ºç«‹ç½‘ç»œ
            g_obj = GraphTokenMainClass(train_loader, 5, test_loader,multimodel,valid_loader,flag )

            # è®­ç»ƒä»¥åŠæµ‹è¯•
            best_acc = 0
            best_fold_model_path = ""

            epoch_bar = tqdm(range(num_epoch), desc=f"ğŸŒ€ Training Fold {fold + 1}", leave=False)
            with open(
                    r"/home/phc/PycharmProjects/CIBG_encrtypted_traffic_service_classification/03_Classification/results/USTC/4class/loss.txt",
                    "a") as f:
                f.write(f'Fols{fold}\n')

            patience = 10  # è¿ç»­å¤šå°‘ä¸ª epoch éªŒè¯ç²¾åº¦ä¸æå‡å°±åœæ­¢
            early_stop_counter = 0
            for epoch in epoch_bar:
                with open(
                        r"/home/phc/PycharmProjects/CIBG_encrtypted_traffic_service_classification/03_Classification/results/USTC/4class/loss.txt",
                        "a") as f:
                    f.write(f'Epoch{epoch}\n')
                # print("--------æ­£åœ¨è¿›è¡Œç¬¬----------"+str(epoch)+"æ¬¡è®­ç»ƒ")
                g_obj.train()
                valid_acc = g_obj.predict()
                if valid_acc > best_acc:
                    best_acc = valid_acc
                    early_stop_counter = 0  # é‡ç½®è®¡æ•°å™¨
                    best_fold_model_path = f'/home/phc/PycharmProjects/CIBG_encrtypted_traffic_service_classification/03_Classification/model/USTC/4class/{name}_{fold}.pt'
                    # ä¿å­˜å½“å‰æŠ˜çš„æœ€ä½³æ¨¡å‹
                    g_obj.save_model(best_fold_model_path)
                    print(f"Saved best model for fold {fold + 1} with accuracy: {best_acc:.4f}")
                else:
                    early_stop_counter += 1
                    print(f"No improvement for {early_stop_counter} epoch(s).")

                    # å¦‚æœè¿ç»­è‹¥å¹²è½®éªŒè¯ç²¾åº¦æ²¡æœ‰æå‡ï¼Œåˆ™æå‰åœæ­¢è®­ç»ƒ
                if early_stop_counter >= patience:
                    print(f"Early stopping triggered at epoch {epoch} (no improvement for {patience} epochs)")
                    break

            # æ¯”è¾ƒæ¯ä¸€æŠ˜çš„æœ€ä½³å‡†ç¡®ç‡ï¼Œæ‰¾å‡ºæœ€ä¼˜çš„ä¸€æŠ˜
            if best_acc > best_overall_acc:
                best_overall_acc = best_acc
                best_model_path = best_fold_model_path  # æ›´æ–°æœ€ä¼˜æ¨¡å‹è·¯å¾„
            # æœ€åï¼Œè¾“å‡º10æŠ˜äº¤å‰éªŒè¯ä¸­æœ€ä¼˜çš„æ¨¡å‹å’Œå‡†ç¡®ç‡
            print(
                f"\n\nğŸŒŸ Best model is from fold {best_model_path.split('_')[-2]} with accuracy: {best_overall_acc:.4f}")

            # ä¿å­˜æœ€ç»ˆæœ€ä¼˜æ¨¡å‹
            print(f"Saving the best overall model to {best_model_path}")

            pred_acc, true_list, pred_list, pred_scr = g_obj.test(f'/home/phc/PycharmProjects/CIBG_encrtypted_traffic_service_classification/03_Classification/model/USTC/4class/{name}_{fold}.pt')
            # print(pred_acc, (true_list[0][0]), (pred_list[0][0]), pred_scr[0])
            # 1.0 3 3[-58.08167266845703, -81.5223388671875, -69.7474594116211, 0.0, -63.356449127197266]

            true_list = [i for item in true_list for i in item]
            pred_list = [i for item in pred_list for i in item]
            true_all.extend(true_list)
            pred_all.extend(pred_list)
            pred_scr_all.extend(pred_scr)

        # è·å–åˆ†ç±»æŠ¥å‘Šï¼ˆæ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡ï¼‰
        conf_matrix = confusion_matrix(true_all, pred_all)
        np.savetxt(r"/home/phc/PycharmProjects/CIBG_encrtypted_traffic_service_classification/03_Classification/results/USTC/4class/confusion_matrix.txt",conf_matrix,fmt="%d")
        FP = conf_matrix.sum(axis=0) - np.diag(conf_matrix)  # å‡é˜³æ€§
        TN = conf_matrix.sum() - (FP + conf_matrix.sum(axis=1) - np.diag(conf_matrix) + np.diag(conf_matrix))  # çœŸé˜´æ€§

        report_dict = classification_report(true_all, pred_all, digits=5, output_dict=True)

        class_fprs = []
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ accuracy
        class_accuracies = {}
        num_classes = len(conf_matrix)
        for i in range(num_classes):
            class_acc = conf_matrix[i, i] / conf_matrix[i, :].sum()
            class_fpr = FP[i] / (FP[i] + TN[i] + 1e-6)  # é¿å…é™¤0
            report_dict[str(i)]['fpr'] = class_fpr
            class_fprs.append(class_fpr)

        overall_acc = accuracy_score(true_all, pred_all)
        overall_prec = precision_score(true_all, pred_all, average='micro')
        overall_recall = recall_score(true_all, pred_all, average='micro')
        overall_f1 = f1_score(true_all, pred_all, average='micro')

        report_dict['overall'] = {
            'precision': overall_prec,
            'recall': overall_recall,
            'f1-score': overall_f1,
            'accuracy': overall_acc,
            'support': len(true_all)
        }

        # ç”¨æ··æ·†çŸ©é˜µé‡æ–°è®¡ç®—æ¯ç±»çš„å‡†ç¡®ç‡ï¼Œå–å®å¹³å‡
        class_accuracies = [conf_matrix[i, i] / (conf_matrix[i, :].sum() + 1e-6) for i in range(num_classes)]
        macro_accuracy = sum(class_accuracies) / num_classes
        report_dict['macro_accuracy'] = {'accuracy': macro_accuracy}
        # æ¯ç±»æ ·æœ¬æ•°
        supports = np.array([report_dict[str(i)]['support'] for i in range(num_classes)])
        # åŠ æƒå¹³å‡ accuracy
        weighted_accuracy = np.sum(np.array(class_accuracies) * supports) / np.sum(supports)
        report_dict['weighted_accuracy'] = {'accuracy': weighted_accuracy}

        FP_total = FP.sum()
        TN_total = TN.sum()
        overall_fpr = FP_total / (FP_total + TN_total + 1e-6)
        report_dict['overall']['fpr'] = overall_fpr
        macro_fpr = np.mean(class_fprs)
        weighted_fpr = np.sum(np.array(class_fprs) * supports) / np.sum(supports)
        report_dict['macro_fpr'] = {'fpr': macro_fpr}
        report_dict['weighted_fpr'] = {'fpr': weighted_fpr}
        # è½¬æ¢æˆ DataFrame å¹¶ä¿å­˜
        class_df = pd.DataFrame(report_dict).transpose()
        class_output_csv_path = f'/home/phc/PycharmProjects/CIBG_encrtypted_traffic_service_classification/03_Classification/results/USTC/4class/{name}_classification_report.csv'
        # class_df.to_csv(class_output_csv_path)
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        class_df.insert(0, 'model_name', name)
        class_df.insert(1, 'timestamp', timestamp)
        class_df.insert(2, 'epoch', num_epoch)

        # è¿½åŠ ä¸€ä¸ªç©ºè¡Œï¼ˆåˆ†éš”ä¸Šæ¬¡çš„è®°å½•ï¼‰
        with open(class_output_csv_path, 'a', encoding='utf-8') as f:
            f.write('\n')  # å¯é€‰ï¼šå†™å…¥ç©ºè¡Œä½œä¸ºåˆ†éš”

        # å†™å…¥CSVï¼Œä½¿ç”¨è¿½åŠ æ¨¡å¼ï¼Œå§‹ç»ˆä¿ç•™ header
        class_df.to_csv(class_output_csv_path, mode='a', header=True, index=True)



if __name__ == '__main__':

    def seed_everything(seed=42):
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False  # ç¡®ä¿ç»“æœå¯é‡å¤
        torch.use_deterministic_algorithms(True)


    seed_everything(42)

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    print(os.getenv("CUBLAS_WORKSPACE_CONFIG"))
    print(device)

    print(torch.cuda.device_count())

    # ZEAC
    # interclassgap_token = 0.23862391757208523
    # interclassgap_graph = 0.910086862395072

    #VNAT
    # interclassgap_token = 0.42514073437876676
    # interclassgap_graph = 0.6962073720100967

    # USTC
    interclassgap_token = 0.2680087036605987
    interclassgap_graph = 0.8937181233957291


    # USTC
    # interclassgap_token = 0.188403141179859
    # interclassgap_graph = 0.6426967860087783


    multimodel = create_model(device, interclassgap_graph, interclassgap_token)
    multimodel=torch.nn.DataParallel(multimodel, device_ids=range(torch.cuda.device_count()))
    flag_map = {
        'GraphSAGE_Transformer_': 1
    }

    for model_name in flag_map:
        major_exp_multi_classify(multimodel.to(device),name=model_name, flag=flag_map[model_name])
